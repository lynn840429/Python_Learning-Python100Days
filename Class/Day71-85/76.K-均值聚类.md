# K-均值聚类

## 聚类与分类的区别

### 分类
类别是已知的，通过对已知分类的数据进行训练和学习，找到这些不同类的特征，再对未分类的数据进行分类。属于监督学习。

### 聚类
事先不知道数据会分为几类，通过聚类分析将数据聚合成几个群体。聚类不需要对数据进行训练和学习。属于无监督学习。

聚類（Clustering）是一種無監督學習 (unsupervised learning)，簡單地說就是把相似的對象歸到同一簇中。簇內的對象越相似，聚類的效果越好。

聚類分析（cluster analysis）試圖將相似對象歸入同一簇，將不相似對象歸到不同簇。K-均值（K-means）會根據事先選擇K個初始質心（centroid）進行聚類，即得到k個簇，且不同簇的中心採用簇中所含有的均值計算而成。

## k-means 聚类

聚类算法有很多种，K-Means 是聚类算法中的最常用的一种，算法最大的特点是简单，好理解，运算速度快，但是只能应用于连续型的数据，并且一定要在聚类前需要手工指定要分成几类。

K-Means 聚类算法的大致意思就是“物以类聚，人以群分”：

1. 首先输入 k 的值，即我们指定希望通过聚类得到 k 个分组；
2. 从数据集中随机选取 k 个数据点作为初始大佬（质心）；
3. 对集合中每一个小弟，计算与每一个大佬的距离，离哪个大佬距离近，就跟定哪个大佬。
4. 这时每一个大佬手下都聚集了一票小弟，这时候召开选举大会，每一群选出新的大佬（即通过算法选出新的质心）。
5. 如果新大佬和老大佬之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），可以认为我们进行的聚类已经达到期望的结果，算法终止。
6. 如果新大佬和老大佬距离变化很大，需要迭代3~5步骤。

系統聚類法需要計算不同樣品或變數的距離，當樣本量很大時，會佔據非常大的計算機記憶體空間，Kmeans是一種快速聚類法，該方法簡單易懂，對計算機要求不高

## K均值原理與計算
![](https://img-blog.csdnimg.cn/20181031185058411.png)
其中E是資料中所有物件與相應聚類中心的均方差之和，p為代表物件空間中的一個點，mi是Ci的均值。

* 时间复杂度：O(tknm)，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点维度。
* 空间复杂度：O(m(n+k))，其中，k 为簇的数目，m 为样本点维度，n 为样本点数。

### 优点
1. 容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；
2. 处理大数据集的时候，该算法可以保证较好的伸缩性；
3. 当簇近似高斯分布的时候，效果非常不错；
4. 算法复杂度低。
### 缺点
1. K 值需要人为设定，不同 K 值得到的结果不一样；
2. 对初始的簇中心敏感，不同选取方式会得到不同结果；
3. 对异常值敏感；
4. 样本只能归为一类，不适合多分类任务；
5. 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。

针对 K-means 算法的缺点，我们可以有很多种调优方式：如数据预处理（去除异常点），合理选择 K 值，高维映射等。

### 数据预处理
K-means 的本质是基于欧式距离的数据划分算法，均值和方差大的维度将对数据的聚类产生决定性影响。所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。常见的数据预处理方式有：数据归一化，数据标准化。

此外，离群点或者噪声数据会对均值产生较大的影响，导致中心偏移，因此我们还需要对数据进行异常点检测。

### 合理选择 K 值
K 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：手肘法、Gap statistic 方法。

### 采用核函数
基于欧式距离的 K-means 假设了了各个数据簇的数据具有一样的的先验概率并呈现球形分布，但这种分布在实际生活中并不常见。面对非凸的数据分布形状时我们可以引入核函数来优化，这时算法又称为核 K-means 算法，是核聚类方法的一种。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。

### K-means++
我们知道初始值的选取对结果的影响很大，对初始值选择的改进是很重要的一部分。在所有的改进算法中，K-means++ 最有名。
![](https://i.imgur.com/L3eSZt1.png)

---
## URL
1. https://blog.csdn.net/huangfei711/article/details/78480078
2. https://zhuanlan.zhihu.com/p/78798251